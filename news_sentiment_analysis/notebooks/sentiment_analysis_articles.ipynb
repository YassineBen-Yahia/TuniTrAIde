{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e831440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import libraries\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b436d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Load the main scraped results\n",
    "print(\"=\"*60)\n",
    "print(\"SENTIMENT ANALYSIS ON SCRAPED ARTICLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "input_file = '../data/processed/global_cleaned.json'\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    full_data = json.load(f)\n",
    "\n",
    "# Extract articles from the structure\n",
    "articles = full_data.get('all_articles', [])\n",
    "print(f\"‚úì Loaded {len(articles)} articles for analysis\")\n",
    "\n",
    "# Preview structure\n",
    "if articles:\n",
    "    print(\"\\nSample article structure:\")\n",
    "    sample = articles[0]\n",
    "    print(f\"Keys: {list(sample.keys())}\")\n",
    "    print(f\"Sample headline: {sample.get('headline', 'N/A')[:80]}...\")\n",
    "    print(f\"Sample tickers: {sample.get('tickers', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef052ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Initialize the sentiment analysis pipeline\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_path = \"../src/models/xlm-roberta-tunisian-finance-final\"\n",
    "\n",
    "print(f\"\\nü§ñ Loading trained model from: {model_path}\")\n",
    "\n",
    "# Create sentiment pipeline\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model_path,\n",
    "    tokenizer=model_path,\n",
    "    device=0 if torch.cuda.is_available() else -1,  # Use GPU if available\n",
    "    max_length=256,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print(\"‚úì Model loaded successfully!\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02436840",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {-1: 'Negative', 0: 'Neutral', 1: 'Positive'}\n",
    "\n",
    "def parse_model_output(result):\n",
    "    \"\"\"\n",
    "    Convert model prediction to sentiment score.\n",
    "    The model already returns -1/0/1 directly, no mapping needed!\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(result, dict):\n",
    "            label = result.get('label')\n",
    "            \n",
    "            # Label is already the sentiment score (-1, 0, or 1)\n",
    "            if isinstance(label, (int, float)):\n",
    "                # Ensure it's a valid sentiment score\n",
    "                if label in [-1, 0, 1]:\n",
    "                    return int(label)\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è Unexpected label value: {label}\")\n",
    "                    return 0\n",
    "            \n",
    "            # Fallback: string format \"LABEL_X\"\n",
    "            elif isinstance(label, str):\n",
    "                if label == 'LABEL_0' or label == '-1':\n",
    "                    return -1\n",
    "                elif label == 'LABEL_1' or label == '0':\n",
    "                    return 0\n",
    "                elif label == 'LABEL_2' or label == '1':\n",
    "                    return 1\n",
    "        \n",
    "        # Direct integer/float\n",
    "        elif isinstance(result, (int, float)):\n",
    "            if result in [-1, 0, 1]:\n",
    "                return int(result)\n",
    "        \n",
    "        return 0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Parse error: {e}, result={result}\")\n",
    "        return 0\n",
    "\n",
    "print(\"‚úì Label mapping configured - NO INVERSE MAPPING!\")\n",
    "\n",
    "# Test it\n",
    "test_cases = [\n",
    "    {'label': -1, 'score': 0.95},  # Negative\n",
    "    {'label': 0, 'score': 0.85},   # Neutral\n",
    "    {'label': 1, 'score': 0.73},   # Positive\n",
    "]\n",
    "\n",
    "for test in test_cases:\n",
    "    parsed = parse_model_output(test)\n",
    "    print(f\"{test} ‚Üí {parsed} ({labels_map[parsed]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1a7838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Return both labels and confidence scores\n",
    "def predict_sentiments_batch(texts, batch_size=16, desc=\"Processing\"):\n",
    "    \"\"\"\n",
    "    Process texts in batches for efficiency.\n",
    "    Returns: (list of sentiment scores, list of confidence scores)\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    sentiment_scores = []\n",
    "    confidence_scores = []\n",
    "    \n",
    "    # Progress bar\n",
    "    num_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), total=num_batches, desc=desc):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        \n",
    "        # Prepare batch (handle empty/None texts)\n",
    "        processed_batch = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        for idx, text in enumerate(batch):\n",
    "            if text and isinstance(text, str) and len(text.strip()) >= 5:\n",
    "                processed_batch.append(text[:512])\n",
    "                valid_indices.append(idx)\n",
    "        \n",
    "        # Initialize with neutral/low confidence\n",
    "        batch_sentiments = [0] * len(batch)\n",
    "        batch_confidences = [0.0] * len(batch)\n",
    "        \n",
    "        try:\n",
    "            if processed_batch:\n",
    "                results = sentiment_analyzer(processed_batch)\n",
    "                \n",
    "                # Map results back to original batch positions\n",
    "                for result_idx, batch_idx in enumerate(valid_indices):\n",
    "                    result = results[result_idx]\n",
    "                    sentiment_score = parse_model_output(result)\n",
    "                    # Get the confidence score from the model\n",
    "                    conf_score = result.get('score', 0.5) if isinstance(result, dict) else 0.5\n",
    "                    \n",
    "                    batch_sentiments[batch_idx] = sentiment_score\n",
    "                    batch_confidences[batch_idx] = conf_score\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Batch error at index {i}: {str(e)}\")\n",
    "        \n",
    "        sentiment_scores.extend(batch_sentiments)\n",
    "        confidence_scores.extend(batch_confidences)\n",
    "    \n",
    "    return sentiment_scores, confidence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Extract and prepare texts from articles\n",
    "print(\"\\nüìù Preparing texts for analysis...\")\n",
    "\n",
    "# Handle both string and list formats for content\n",
    "headlines = []\n",
    "contents = []\n",
    "\n",
    "for article in articles:\n",
    "    # Extract headline\n",
    "    headline = article.get('headline', '')\n",
    "    headlines.append(headline if headline else '')\n",
    "    \n",
    "    # Extract content (handle different formats)\n",
    "    content = article.get('content', '')\n",
    "    if isinstance(content, list):\n",
    "        # Join list of paragraphs\n",
    "        content = ' '.join([p for p in content if p])\n",
    "    elif not isinstance(content, str):\n",
    "        content = str(content) if content else ''\n",
    "    \n",
    "    contents.append(content)\n",
    "\n",
    "print(f\"‚úì Prepared {len(headlines)} headlines\")\n",
    "print(f\"‚úì Prepared {len(contents)} content texts\")\n",
    "\n",
    "# Validate\n",
    "empty_headlines = sum(1 for h in headlines if not h or len(h.strip()) < 5)\n",
    "empty_contents = sum(1 for c in contents if not c or len(c.strip()) < 5)\n",
    "print(f\"  - Empty/short headlines: {empty_headlines}\")\n",
    "print(f\"  - Empty/short contents: {empty_contents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecba5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: UPDATED - Get both scores and confidences\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING SENTIMENT ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nü§ñ Analyzing Headlines (Primary Signal)...\")\n",
    "headline_scores, headline_confidences = predict_sentiments_batch(headlines, batch_size=16, desc=\"Headlines\")\n",
    "\n",
    "print(\"\\nüîç Analyzing Content (Detailed Context)...\")\n",
    "content_scores, content_confidences = predict_sentiments_batch(contents, batch_size=16, desc=\"Content\")\n",
    "\n",
    "print(\"\\n‚úÖ Inference completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a77d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: ENHANCED - Combine model confidence with agreement logic\n",
    "print(\"\\n‚öôÔ∏è Applying sentiment aggregation logic...\")\n",
    "\n",
    "for i, article in enumerate(articles):\n",
    "    h_score = headline_scores[i]\n",
    "    c_score = content_scores[i]\n",
    "    h_conf = headline_confidences[i]\n",
    "    c_conf = content_confidences[i]\n",
    "    \n",
    "    # COMBINED CONFIDENCE CALCULATION:\n",
    "    # Base confidence = model's confidence score\n",
    "    # Boost if both agree, penalize if they disagree\n",
    "    \n",
    "    if h_score != 0:\n",
    "        # Headline has clear sentiment\n",
    "        if c_score == h_score:\n",
    "            # Both agree - use max confidence, boost by 10%\n",
    "            final_sentiment = h_score\n",
    "            confidence = min(1.0, max(h_conf, c_conf) * 1.1)\n",
    "        elif c_score == 0:\n",
    "            # Content is neutral - use headline confidence as-is\n",
    "            final_sentiment = h_score\n",
    "            confidence = h_conf\n",
    "        else:\n",
    "            # They disagree - use headline but penalize confidence by 30%\n",
    "            final_sentiment = h_score\n",
    "            confidence = h_conf * 0.7\n",
    "    else:\n",
    "        # Headline is neutral - use content\n",
    "        final_sentiment = c_score\n",
    "        if c_score != 0:\n",
    "            confidence = c_conf * 0.8  # Slight penalty for neutral headline\n",
    "        else:\n",
    "            confidence = max(h_conf, c_conf) * 0.5  # Both neutral - low confidence\n",
    "    \n",
    "    # Store enriched results\n",
    "    article['sentiment_score'] = int(final_sentiment)\n",
    "    #article['sentiment_label'] = labels_map[final_sentiment]\n",
    "    #article['headline_sentiment'] = int(h_score)\n",
    "    #article['content_sentiment'] = int(c_score)\n",
    "    #article['headline_confidence'] = round(h_conf, 3)\n",
    "    #article['content_confidence'] = round(c_conf, 3)\n",
    "    article['confidence'] = round(confidence, 3)\n",
    "\n",
    "print(\"‚úÖ Sentiment enrichment complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cdb2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Preview results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, a in enumerate(articles[:5]):\n",
    "    print(f\"\\n[Article {i+1}]\")\n",
    "    print(f\"Date: {a.get('date', 'N/A')}\")\n",
    "    print(f\"Source: {a.get('source', 'N/A')}\")\n",
    "    print(f\"Tickers: {a.get('tickers', 'N/A')}\")\n",
    "    print(f\"Headline: {a.get('headline', '')[:80]}...\")\n",
    "    print(f\"  ‚îî‚îÄ Headline Sentiment: {labels_map[a['headline_sentiment']]}\")\n",
    "    print(f\"  ‚îî‚îÄ Content Sentiment: {labels_map[a['content_sentiment']]}\")\n",
    "    print(f\"  ‚îî‚îÄ Final Sentiment: {a['sentiment_label']} (confidence: {a['confidence']})\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25: Sentiment distribution analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sentiment_counts = pd.Series([a['sentiment_score'] for a in articles]).value_counts().sort_index()\n",
    "confidence_counts = pd.Series([a['confidence'] for a in articles]).value_counts()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SENTIMENT DISTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "total = len(articles)\n",
    "for score in [-1, 0, 1]:\n",
    "    count = sentiment_counts.get(score, 0)\n",
    "    label = labels_map[score]\n",
    "    pct = (count / total) * 100\n",
    "    # Calculate average confidence for this sentiment\n",
    "    avg_conf = np.mean([a['confidence'] for a in articles if a['sentiment_score'] == score])\n",
    "    print(f\"{label:>10}: {count:>4} ({pct:>5.1f}%) | Avg Conf: {avg_conf:.3f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nCONFIDENCE STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "all_confidences = [a['confidence'] for a in articles]\n",
    "print(f\"  Mean: {np.mean(all_confidences):.3f}\")\n",
    "print(f\"Median: {np.median(all_confidences):.3f}\")\n",
    "print(f\"   Min: {np.min(all_confidences):.3f}\")\n",
    "print(f\"   Max: {np.max(all_confidences):.3f}\")\n",
    "print(f\"   Std: {np.std(all_confidences):.3f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# High confidence threshold analysis\n",
    "high_conf_threshold = 0.8\n",
    "high_conf_count = sum(1 for c in all_confidences if c >= high_conf_threshold)\n",
    "print(f\"\\nHigh confidence (‚â•{high_conf_threshold}): {high_conf_count} ({high_conf_count/total*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312251d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26: Visualize distributions\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Sentiment distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "colors = ['#ff6b6b', '#95a5a6', '#51cf66']\n",
    "bars = plt.bar(\n",
    "    ['Negative', 'Neutral', 'Positive'],\n",
    "    [sentiment_counts.get(-1, 0), sentiment_counts.get(0, 0), sentiment_counts.get(1, 0)],\n",
    "    color=colors,\n",
    "    edgecolor='black',\n",
    "    linewidth=1.5\n",
    ")\n",
    "plt.title('Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Number of Articles', fontsize=11)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(height)}',\n",
    "            ha='center', va='bottom', fontweight='bold')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07221a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 27: Analyze disagreements\n",
    "disagreements = [\n",
    "    a for a in articles \n",
    "    if a['headline_sentiment'] != 0 \n",
    "    and a['content_sentiment'] != 0 \n",
    "    and a['headline_sentiment'] != a['content_sentiment']\n",
    "]\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Found {len(disagreements)} articles with headline-content disagreement\")\n",
    "\n",
    "if disagreements:\n",
    "    print(\"\\nSample disagreements:\")\n",
    "    print(\"=\"*80)\n",
    "    for i, a in enumerate(disagreements[:3]):\n",
    "        print(f\"\\n[{i+1}] {a.get('headline', '')[:70]}...\")\n",
    "        print(f\"    Headline: {labels_map[a['headline_sentiment']]}\")\n",
    "        print(f\"    Content: {labels_map[a['content_sentiment']]}\")\n",
    "        print(f\"    Final: {a['sentiment_label']} (confidence: {a['confidence']})\")\n",
    "        print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c622338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 28: Save enriched articles data\n",
    "output_file = '../exports/articles_with_sentiment_2021_2026.json'\n",
    "\n",
    "enriched_data = {\n",
    "    'metadata': {\n",
    "        'total_articles': len(articles),\n",
    "        'analysis_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'model_used': 'xlm-roberta-tunisian-finance',\n",
    "        'sentiment_distribution': {\n",
    "            'negative': int(sentiment_counts.get(-1, 0)),\n",
    "            'neutral': int(sentiment_counts.get(0, 0)),\n",
    "            'positive': int(sentiment_counts.get(1, 0))\n",
    "        },\n",
    "        'confidence_distribution': {\n",
    "            'high': int(confidence_counts.get('high', 0)),\n",
    "            'medium': int(confidence_counts.get('medium', 0)),\n",
    "            'low': int(confidence_counts.get('low', 0))\n",
    "        },\n",
    "        'disagreements': len(disagreements)\n",
    "    },\n",
    "    'articles': articles\n",
    "}\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(enriched_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Enriched articles saved to: {output_file}\")\n",
    "print(f\"   Total size: {len(json.dumps(enriched_data, ensure_ascii=False))} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601268c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 29: Generate ticker-level sentiment aggregation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING TICKER-LEVEL SIGNALS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ticker_sentiments = {}\n",
    "\n",
    "for article in articles:\n",
    "    # Extract tickers (handle different formats)\n",
    "    tickers = article.get('tickers', [])\n",
    "    \n",
    "    # Convert to list if string\n",
    "    if isinstance(tickers, str):\n",
    "        if tickers:\n",
    "            tickers = [t.strip() for t in tickers.split(',')]\n",
    "        else:\n",
    "            tickers = []\n",
    "    elif not isinstance(tickers, list):\n",
    "        tickers = []\n",
    "    \n",
    "    sentiment = article['sentiment_score']\n",
    "    confidence = article['confidence']\n",
    "    date = article.get('date', '')\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        ticker = ticker.strip()\n",
    "        if not ticker:\n",
    "            continue\n",
    "            \n",
    "        if ticker not in ticker_sentiments:\n",
    "            ticker_sentiments[ticker] = {\n",
    "                'ticker': ticker,\n",
    "                'positive_count': 0,\n",
    "                'neutral_count': 0,\n",
    "                'negative_count': 0,\n",
    "                'total_articles': 0,\n",
    "                'net_sentiment': 0.0,\n",
    "                'high_confidence_count': 0,\n",
    "                'recent_articles': []\n",
    "            }\n",
    "        \n",
    "        stats = ticker_sentiments[ticker]\n",
    "        stats['total_articles'] += 1\n",
    "        \n",
    "        # Count by sentiment\n",
    "        if sentiment == 1:\n",
    "            stats['positive_count'] += 1\n",
    "        elif sentiment == -1:\n",
    "            stats['negative_count'] += 1\n",
    "        else:\n",
    "            stats['neutral_count'] += 1\n",
    "        \n",
    "        # Track confidence\n",
    "        if confidence == 'high':\n",
    "            stats['high_confidence_count'] += 1\n",
    "        \n",
    "        # Store recent article info\n",
    "        stats['recent_articles'].append({\n",
    "            'date': date,\n",
    "            'headline': article.get('headline', '')[:100],\n",
    "            'sentiment': labels_map[sentiment],\n",
    "            'confidence': confidence\n",
    "        })\n",
    "\n",
    "# Calculate net sentiment scores\n",
    "for ticker, stats in ticker_sentiments.items():\n",
    "    if stats['total_articles'] > 0:\n",
    "        # Net sentiment: (positive - negative) / total\n",
    "        stats['net_sentiment'] = round(\n",
    "            (stats['positive_count'] - stats['negative_count']) / stats['total_articles'],\n",
    "            3\n",
    "        )\n",
    "        \n",
    "        # Keep only 5 most recent articles\n",
    "        stats['recent_articles'] = sorted(\n",
    "            stats['recent_articles'],\n",
    "            key=lambda x: x['date'],\n",
    "            reverse=True\n",
    "        )[:5]\n",
    "\n",
    "print(f\"‚úì Analyzed {len(ticker_sentiments)} unique tickers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c69debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 30: Export ticker signals to JSON\n",
    "signals_output = '../exports/ticker_sentiment_signals.json'\n",
    "\n",
    "ticker_signals = {\n",
    "    'generated_at': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'model': 'xlm-roberta-tunisian-finance',\n",
    "    'total_tickers': len(ticker_sentiments),\n",
    "    'total_articles_analyzed': len(articles),\n",
    "    'tickers': ticker_sentiments\n",
    "}\n",
    "\n",
    "with open(signals_output, 'w', encoding='utf-8') as f:\n",
    "    json.dump(ticker_signals, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Ticker signals saved to: {signals_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dea1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 31: Display top tickers by net sentiment\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 10 TICKERS BY NET SENTIMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sorted_tickers = sorted(\n",
    "    ticker_sentiments.items(),\n",
    "    key=lambda x: x[1]['net_sentiment'],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "for i, (ticker, stats) in enumerate(sorted_tickers[:10], 1):\n",
    "    print(f\"\\n{i}. {ticker}\")\n",
    "    print(f\"   Total Articles: {stats['total_articles']}\")\n",
    "    print(f\"   Positive: {stats['positive_count']} | Neutral: {stats['neutral_count']} | Negative: {stats['negative_count']}\")\n",
    "    print(f\"   Net Sentiment: {stats['net_sentiment']:+.3f}\")\n",
    "    print(f\"   High Confidence: {stats['high_confidence_count']}/{stats['total_articles']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BOTTOM 5 TICKERS BY NET SENTIMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, (ticker, stats) in enumerate(sorted_tickers[-5:], 1):\n",
    "    print(f\"\\n{i}. {ticker}\")\n",
    "    print(f\"   Total Articles: {stats['total_articles']}\")\n",
    "    print(f\"   Positive: {stats['positive_count']} | Neutral: {stats['neutral_count']} | Negative: {stats['negative_count']}\")\n",
    "    print(f\"   Net Sentiment: {stats['net_sentiment']:+.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
