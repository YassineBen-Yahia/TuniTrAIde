{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aae157a",
   "metadata": {},
   "source": [
    "# 1. Setup & Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b0dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\medam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete.\n"
     ]
    }
   ],
   "source": [
    "# !pip install setfit datasets pandas\n",
    "\n",
    "# 1. Force install a compatible version of transformers\n",
    "#%pip install \"transformers<4.48.0\" \"setfit>=1.1.0\" datasets accelerate\n",
    "\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from setfit import SetFitModel, SetFitTrainer, TrainingArguments\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "\n",
    "print(\"Setup Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b064e91a",
   "metadata": {},
   "source": [
    "# 2. Load the Labeled Dataset\n",
    "\n",
    "Load the fewshot_sentiment_dataset.json file you prepared. This file contains the \"Gold Examples\" used to teach the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46556b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 examples for training.\n",
      "Example: {'text': 'la bna bank d√©croche trois gold awards aux tunisia digital awards 2026', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# Load your labeled few-shot dataset\n",
    "with open('../data/reference/fewshot_sentiment_dataset.json', 'r', encoding='utf-8') as f:\n",
    "    labeled_data = json.load(f)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "# Expects a list like: [{\"text\": \"...\", \"label\": 1}, ...]\n",
    "train_dataset = Dataset.from_list(labeled_data)\n",
    "\n",
    "print(f\"Loaded {len(train_dataset)} examples for training.\")\n",
    "print(f\"Example: {train_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2203002",
   "metadata": {},
   "source": [
    "# 3. Initialize the Multilingual Model\n",
    "\n",
    "Load the pre-trained \"Backbone\" model. We use paraphrase-multilingual-MiniLM-L12-v2 because it understands both French and Arabic, which is essential for the Tunisian market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c0ddab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained multilingual model\n",
    "model_id = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "model = SetFitModel.from_pretrained(\n",
    "    model_id,\n",
    "    labels=[-1, 0, 1]  # -1: Neg, 0: Neu, 1: Pos\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d1aec2",
   "metadata": {},
   "source": [
    "# 4. Fine-Tuning (Few-Shot Training)\n",
    "\n",
    "Train the model using the SetFitTrainer. This step uses Contrastive Learning to help the model distinguish between positive and negative financial signals with very few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdccef4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\medam\\AppData\\Local\\Temp\\ipykernel_26948\\1376469582.py:2: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n",
      "  trainer = SetFitTrainer(\n",
      "Applying column mapping to the training dataset\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 2304.92 examples/s]\n",
      "***** Running training *****\n",
      "  Num unique pairs = 4000\n",
      "  Batch size = 16\n",
      "  Num epochs = 3\n",
      "  0%|          | 0/750 [00:00<?, ?it/s]c:\\Users\\medam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      "  0%|          | 1/750 [00:03<45:55,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.3732, 'grad_norm': 4.242681503295898, 'learning_rate': 2.666666666666667e-07, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 50/750 [02:07<29:34,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.2314, 'grad_norm': 2.920628309249878, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|‚ñà‚ñé        | 100/750 [04:13<27:40,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0835, 'grad_norm': 0.23106688261032104, 'learning_rate': 1.925925925925926e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 150/750 [06:13<21:47,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0024, 'grad_norm': 0.053406063467264175, 'learning_rate': 1.7777777777777777e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|‚ñà‚ñà‚ñã       | 200/750 [08:03<20:11,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.001, 'grad_norm': 0.07318763434886932, 'learning_rate': 1.6296296296296297e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 250/750 [09:53<18:12,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0005, 'grad_norm': 0.08175420016050339, 'learning_rate': 1.4814814814814815e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 300/750 [11:52<18:50,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0004, 'grad_norm': 0.07314854860305786, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 350/750 [13:56<16:38,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0004, 'grad_norm': 0.04499372839927673, 'learning_rate': 1.1851851851851852e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 400/750 [15:56<13:59,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0004, 'grad_norm': 0.024713490158319473, 'learning_rate': 1.037037037037037e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 450/750 [17:54<11:48,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0003, 'grad_norm': 0.04250386357307434, 'learning_rate': 8.888888888888888e-06, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 500/750 [19:53<09:43,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0003, 'grad_norm': 0.041771795600652695, 'learning_rate': 7.4074074074074075e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\medam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  super().__init__(loader)\n",
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 550/750 [21:57<07:47,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0002, 'grad_norm': 0.030479446053504944, 'learning_rate': 5.925925925925926e-06, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 600/750 [24:01<06:44,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0002, 'grad_norm': 0.03718612715601921, 'learning_rate': 4.444444444444444e-06, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 650/750 [26:02<04:10,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0002, 'grad_norm': 0.03574511036276817, 'learning_rate': 2.962962962962963e-06, 'epoch': 2.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 700/750 [28:09<02:08,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0002, 'grad_norm': 0.03477509692311287, 'learning_rate': 1.4814814814814815e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 750/750 [30:19<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0002, 'grad_norm': 0.024035964161157608, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 750/750 [30:25<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1825.6912, 'train_samples_per_second': 6.573, 'train_steps_per_second': 0.411, 'train_loss': 0.021634811465938885, 'epoch': 3.0}\n",
      "Model trained and saved locally.\n"
     ]
    }
   ],
   "source": [
    "# Configure the trainer\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    loss_class=CosineSimilarityLoss,\n",
    "    batch_size=16,\n",
    "    num_epochs=3,\n",
    "    num_iterations=40, # Higher iterations = better learning for few-shot\n",
    "    column_mapping={\"text\": \"text\", \"label\": \"label\"}\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the specialized model\n",
    "model_path = \"models/tunisian_finance_model\"\n",
    "model = SetFitModel.from_pretrained(model_path)\n",
    "print(\"Model trained and saved locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b66f6f",
   "metadata": {},
   "source": [
    "# 5. Inference on Scraped Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d2ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles to analyze: 97\n",
      "ü§ñ Analyzing Headlines (Primary Signal)...\n",
      "üîç Analyzing Content (Detailed Context)...\n",
      "‚úÖ Inference completed. Each article now has a 'sentiment_score'.\n",
      "\n",
      "Ticker: ['SFBT'] | Score: 0\n",
      "Headline: la sfbt annonce plus de 840 millions de dinars de ...\n",
      "\n",
      "Ticker: ['BNA'] | Score: 0\n",
      "Headline: la bna bank d√©croche trois gold awards aux tunisia...\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the main scraped results\n",
    "input_file = '../data/processed/global_cleaned.json'\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    full_data = json.load(f)\n",
    "\n",
    "articles = full_data['all_articles']\n",
    "\n",
    "# 2. Extract texts for the model\n",
    "# We prepare two lists: one for headlines, one for the full content\n",
    "headlines = [a.get('headline', '') for a in articles]\n",
    "contents = [a.get('content', '') for a in articles]\n",
    "\n",
    "print(f\"Total articles to analyze: {len(articles)}\")\n",
    "\n",
    "# 3. Run Predictions\n",
    "print(\"ü§ñ Analyzing Headlines (Primary Signal)...\")\n",
    "headline_preds = model.predict(headlines)\n",
    "\n",
    "print(\"üîç Analyzing Content (Detailed Context)...\")\n",
    "content_preds = model.predict(contents)\n",
    "\n",
    "# 4. Apply Weighted Logic\n",
    "# Strategy: Headline is the boss. If headline is neutral, we check the content for details.\n",
    "# This prevents \"noise\" in the content from ruining a clear headline.\n",
    "\n",
    "for i, article in enumerate(articles):\n",
    "    h_score = int(headline_preds[i])\n",
    "    c_score = int(content_preds[i])\n",
    "    \n",
    "    # LOGIC:\n",
    "    # If the headline has a strong sentiment (1 or -1), we trust it.\n",
    "    # If the headline is neutral (0), we let the content decide.\n",
    "    if h_score != 0:\n",
    "        final_sentiment = h_score\n",
    "    else:\n",
    "        final_sentiment = c_score\n",
    "        \n",
    "    # Store the results back in the article object\n",
    "    article['sentiment_score'] = final_sentiment\n",
    "    \n",
    "\n",
    "print(\"‚úÖ Inference completed. Each article now has a 'sentiment_score'.\")\n",
    "\n",
    "# Preview the first few results\n",
    "for a in articles[:2]:\n",
    "    print(f\"\\nTicker: {a['tickers']} | Score: {a['sentiment_score']}\")\n",
    "    print(f\"Headline: {a['headline'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a7d7c",
   "metadata": {},
   "source": [
    "# 6. Export Results for Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18b362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Success! Data formatted for visualization.\n",
      "File saved as: final_sentiment_analysis_results.csv\n",
      "Total rows (after exploding multi-ticker articles): 150\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>SFBT</td>\n",
       "      <td>0</td>\n",
       "      <td>la sfbt annonce plus de 840 millions de dinars...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-31</td>\n",
       "      <td>BNA</td>\n",
       "      <td>0</td>\n",
       "      <td>la bna bank d√©croche trois gold awards aux tun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-29</td>\n",
       "      <td>ATTIJARI BANK</td>\n",
       "      <td>-1</td>\n",
       "      <td>bilan 2025 du secteur du leasing cot√© : r√©sili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-29</td>\n",
       "      <td>STB</td>\n",
       "      <td>-1</td>\n",
       "      <td>la bourse de tunis renforce l‚Äôaccompagnement d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-01-28</td>\n",
       "      <td>STB</td>\n",
       "      <td>-1</td>\n",
       "      <td>le pari r√©ussi de hatem zaara √† la t√™te de la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>BIAT</td>\n",
       "      <td>1</td>\n",
       "      <td>bourse de tunis : le tunindex grignote quelque...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>CARTHAGE CEMENT</td>\n",
       "      <td>1</td>\n",
       "      <td>bourse de tunis : le tunindex grignote quelque...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>SAH</td>\n",
       "      <td>1</td>\n",
       "      <td>bourse de tunis : le tunindex grignote quelque...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>BNA</td>\n",
       "      <td>-1</td>\n",
       "      <td>atl : fitch ratings accorde la note √† long ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2025-12-11</td>\n",
       "      <td>ONE TECH HOLDING</td>\n",
       "      <td>0</td>\n",
       "      <td>ÿ™ŸàŸÇŸäÿπ ÿ≥ÿ®ÿπ ÿßÿ™ŸÅÿßŸÇŸäÿßÿ™ ÿ¥ÿ±ÿßŸÉŸá ÿ®ŸäŸÜ ŸÖÿ§ÿ≥ÿ≥ÿßÿ™ ÿ™ŸàŸÜÿ≥ŸäŸá Ÿàÿ¨ÿ≤...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date            ticker  sentiment_score  \\\n",
       "0   2026-01-31              SFBT                0   \n",
       "1   2026-01-31               BNA                0   \n",
       "2   2026-01-29     ATTIJARI BANK               -1   \n",
       "3   2026-01-29               STB               -1   \n",
       "4   2026-01-28               STB               -1   \n",
       "..         ...               ...              ...   \n",
       "94  2025-12-11              BIAT                1   \n",
       "94  2025-12-11   CARTHAGE CEMENT                1   \n",
       "94  2025-12-11               SAH                1   \n",
       "95  2025-12-11               BNA               -1   \n",
       "96  2025-12-11  ONE TECH HOLDING                0   \n",
       "\n",
       "                                             headline  \n",
       "0   la sfbt annonce plus de 840 millions de dinars...  \n",
       "1   la bna bank d√©croche trois gold awards aux tun...  \n",
       "2   bilan 2025 du secteur du leasing cot√© : r√©sili...  \n",
       "3   la bourse de tunis renforce l‚Äôaccompagnement d...  \n",
       "4   le pari r√©ussi de hatem zaara √† la t√™te de la ...  \n",
       "..                                                ...  \n",
       "94  bourse de tunis : le tunindex grignote quelque...  \n",
       "94  bourse de tunis : le tunindex grignote quelque...  \n",
       "94  bourse de tunis : le tunindex grignote quelque...  \n",
       "95  atl : fitch ratings accorde la note √† long ter...  \n",
       "96  ÿ™ŸàŸÇŸäÿπ ÿ≥ÿ®ÿπ ÿßÿ™ŸÅÿßŸÇŸäÿßÿ™ ÿ¥ÿ±ÿßŸÉŸá ÿ®ŸäŸÜ ŸÖÿ§ÿ≥ÿ≥ÿßÿ™ ÿ™ŸàŸÜÿ≥ŸäŸá Ÿàÿ¨ÿ≤...  \n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Convert the list of articles into a Pandas DataFrame\n",
    "df = pd.DataFrame(articles)\n",
    "\n",
    "# 2. 'Explode' the tickers column\n",
    "# If an article has tickers [\"SFBT\", \"BNA\"], it will create two rows: \n",
    "# one for SFBT and one for BNA, both sharing the same sentiment score.\n",
    "df_exploded = df.explode('tickers')\n",
    "\n",
    "# 3. Clean up the DataFrame\n",
    "# We only keep the columns we need for charts to keep the file small\n",
    "columns_to_keep = ['date', 'tickers', 'sentiment_score', 'headline']\n",
    "df_final = df_exploded[columns_to_keep]\n",
    "\n",
    "# 4. Rename 'tickers' to 'ticker' for clarity\n",
    "df_final = df_final.rename(columns={'tickers': 'ticker'})\n",
    "\n",
    "# 5. Export to CSV\n",
    "# 'utf-8-sig' ensures that Arabic characters open correctly in Excel\n",
    "output_csv = '../exports/final_sentiment_analysis_results.csv'\n",
    "df_final.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"üìä Success! Data formatted for visualization.\")\n",
    "print(f\"File saved as: {output_csv}\")\n",
    "print(f\"Total rows (after exploding multi-ticker articles): {len(df_final)}\")\n",
    "\n",
    "# Preview the structure\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1dccaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Exported: daily_ticker_sentiment_signals.json\n"
     ]
    }
   ],
   "source": [
    "# 6. Export to JSON (Records format)\n",
    "output_json = '../exports/daily_ticker_sentiment_signals.json'\n",
    "\n",
    "# force_ascii=False keeps Arabic/French characters readable\n",
    "# indent=4 makes the file human-readable (good for the hackathon pitch)\n",
    "df_final.to_json(output_json, orient='records', force_ascii=False, indent=4)\n",
    "\n",
    "print(f\"JSON Exported: {output_json}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
